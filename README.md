# Multi-modal-Lie-detection-


Inaccuracies in polygraph tests often lead to wrongful convictions, false information, and bias, which have significant consequences for both the legal and political systems. In light of this, analyzing facial micro-expressions has emerged as a method to detect deception; however, current models have not reached high enough accuracies to be put into use. The purpose of this paper is to aid in remedying these problems. The unique multimodal transformer architecture used in this paper improves upon previous approaches by using auditory input, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participantsâ€™ micro-expressions and mannerisms. Various models were trained for classification instances of lies and truth using these processed and concatenated features. Using this methodology, the CNN Conv1D multimodal model achieved a 95.4\% average accuracy. However, further research is still required to create higher-quality datasets
