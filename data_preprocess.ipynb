{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import WavLMModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the files and saving bert extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files in a folder in a sorted order.\n",
    "def get_files(dir):\n",
    "    files = natsorted(os.listdir(dir))\n",
    "    return [os.path.join(dir, i) for i in files]\n",
    "\n",
    "# fid: file id. \n",
    "def get_fid(file):\n",
    "    return os.path.basename(file).split(\".\")[0]\n",
    "\n",
    "# Make a directory. \n",
    "def mkdir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "        \n",
    "# Write a txt file. \n",
    "def write(file_name, data):\n",
    "    with open(file_name, 'w', encoding='UTF-8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "# Read a txt file.\n",
    "def read(file_name):\n",
    "    with open(file_name, 'r', encoding = 'UTF-8') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "\n",
    "def count_words(data):\n",
    "    return data.count(\" \") + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files = get_files('./Transcription/Deceptive') + get_files('./Transcription/Truthful')\n",
    "len(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/data/lie_detection/clips_umich/text\"\n",
    "mkdir(save_dir)\n",
    "for file in text_files:\n",
    "    fid = get_fid(file)\n",
    "    save_file = os.path.join(save_dir, fid + \".txt\")\n",
    "    data = read(file)\n",
    "    write(save_file, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18., 24., 36., 18., 12.,  6.,  3.,  1.,  0.,  2.]),\n",
       " array([  8. ,  29.4,  50.8,  72.2,  93.6, 115. , 136.4, 157.8, 179.2,\n",
       "        200.6, 222. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "words = []\n",
    "for i in get_files('/data/lie_detection/clips_umich/text'):\n",
    "    data = read(i)\n",
    "    words.append(count_words(data))\n",
    "\n",
    "plt.hist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bert. \n",
    "from transformers import BertTokenizer, BertModel\n",
    "# Initialize tokenizer and model\n",
    "device = torch.device('cuda:7')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "\n",
    "def load_transcription_features(file_path, max_length=128):\n",
    "    try:\n",
    "        # Read transcription with utf-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError: {e}\")\n",
    "        return np.zeros((768,))  # Return a dummy feature vector (BERT output size is 768)\n",
    "\n",
    "    if not text:\n",
    "        print(f\"Error: No text found in transcription file {file_path}\")\n",
    "        return np.zeros((768,))  # Return a dummy feature vector\n",
    "\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length').to(device)\n",
    "\n",
    "    # Extract features using BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    features = outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "\n",
    "    return features.cpu().numpy().flatten()  # Move result back to CPU and convert to numpy array\n",
    "\n",
    "# Define paths\n",
    "deceptive_textfolder = os.path.join(\"Transcription\", \"Deceptive\")\n",
    "truthful_textfolder = os.path.join(\"Transcription\", \"Truthful\")\n",
    "\n",
    "# Initialize the feature extractor\n",
    "# Function to extract features from all videos in a folder\n",
    "def extract_features_from_folder(folder_path, label):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for text_name in os.listdir(folder_path):\n",
    "        text_path = os.path.join(folder_path, text_name)\n",
    "        if text_path.endswith(\".txt\"):  # Ensure it's a text file\n",
    "            text_features = load_transcription_features(text_path)\n",
    "            features.append(text_features)\n",
    "            labels.append(label)\n",
    "    return features, labels\n",
    "\n",
    "deceptive_textfeatures, deceptive_labels = extract_features_from_folder(deceptive_textfolder, label='Deceptive')\n",
    "truthful_textfeatures, truthful_labels = extract_features_from_folder(truthful_textfolder, label='Truthful')\n",
    "\n",
    "# Combine features and labels\n",
    "all_textfeatures = deceptive_textfeatures + truthful_textfeatures\n",
    "all_textlabels = deceptive_labels + truthful_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:02<00:00, 53.74it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"/data/lie_detection/clips_umich/bert_cls\"\n",
    "mkdir(save_dir)\n",
    "for file in tqdm(get_files('/data/lie_detection/clips_umich/text')):\n",
    "    inputs = tokenizer(read(file), \n",
    "                    return_tensors='pt', max_length=512, truncation=True, padding='max_length').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        features = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"{get_fid(file)}.npy\"), features.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class VideoFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet18', pretrained=True):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load a pre-trained model\n",
    "        if model_name == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=pretrained)\n",
    "        elif model_name == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError('Model not supported')\n",
    "\n",
    "        # Remove the last layer\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Define the image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, video_path, fps=4):\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_features = []\n",
    "\n",
    "        # Get the original frame rate of the video\n",
    "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_interval = int(original_fps / fps)\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process every `frame_interval`-th frame\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.transform(frame).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    feature = self.model(frame)\n",
    "                    feature = feature.view(-1).cpu().numpy()\n",
    "                    frame_features.append(feature)\n",
    "\n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        return np.array(frame_features)\n",
    "    \n",
    "# Initialize the feature extractor\n",
    "extractor = VideoFeatureExtractor(model_name='resnet18', pretrained=True)\n",
    "\n",
    "save_dir = \"/data/lie_detection/clips_umich/resnet18\"\n",
    "mkdir(save_dir)\n",
    "\n",
    "for file in tqdm(get_files(\"/data/lie_detection/clips_umich/video\")):\n",
    "    fid = get_fid(file)\n",
    "    save_file = os.path.join(save_dir, fid + \".npy\")\n",
    "    features = extractor.extract_features(file)\n",
    "    np.save(save_file, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw videos are stored at /data/lie_detection/clips_umich/video.\n",
    "# Extract audio from the videos. \n",
    "# The audio files are stored at /data/lie_detection/clips_umich/audio.\n",
    "save_dir = \"/data/lie_detection/clips_umich/audio\"\n",
    "mkdir(save_dir)\n",
    "\n",
    "# Use ffmpeg to extract audio from the videos.\n",
    "# ffmpeg -i input.mp4 -vn -acodec pcm_s16le -ar 44100 -ac 2 output.wav\n",
    "for file in tqdm(get_files(\"/data/lie_detection/clips_umich/video\")):\n",
    "    fid = get_fid(file)\n",
    "    os.system(f\"ffmpeg -hide_banner -loglevel error -i {file} -vn -acodec pcm_s16le -ar 44100 -ac 2 {os.path.join(save_dir, fid)}.wav\")\n",
    "# os.system(f\"ffmpeg -i input.mp4 output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now if you load in the audio, it's 44100 hz.\n",
    "data, sr = sf.read(get_files(\"/data/lie_detection/clips_umich/audio\")[0])\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now each audio has two channels. \n",
    "# Merge the two channels into one. Why? \n",
    "# Because some files appear to have single channel audio, but some have two channels.\n",
    "\n",
    "# Also Convert all wavs into 16khz.\n",
    "for file in tqdm(get_files(\"/data/lie_detection/clips_umich/audio\")):\n",
    "    data, sr = sf.read(file)\n",
    "    assert data.shape[1] == 2\n",
    "    data = data.mean(axis=1)\n",
    "    \n",
    "    data = librosa.resample(data, orig_sr = sr, target_sr = 16000)\n",
    "    sf.write(file, data, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the audio again. Make sure that it's 16khz. \n",
    "data, sr = sf.read(get_files(\"/data/lie_detection/clips_umich/audio\")[0])\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WavLMModel(\n",
       "  (feature_extractor): WavLMFeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): WavLMLayerNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (1-4): 4 x WavLMLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x WavLMLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): WavLMFeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): WavLMEncoderStableLayerNorm(\n",
       "    (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): WavLMSamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): WavLMEncoderLayerStableLayerNorm(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "          (rel_attn_embed): Embedding(320, 16)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-23): 23 x WavLMEncoderLayerStableLayerNorm(\n",
       "        (attention): WavLMAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): WavLMFeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can use WavLM to encode the audio.\n",
    "# Load the model\n",
    "device = torch.device('cuda:7')\n",
    "model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm saving the CNN output from WavLM model (i.e. outputs from layer 0)\n",
    "# Your task will involve exploring other layers as well.\n",
    "# We need to pre-save these features so that during training, \n",
    "# we can directly use them instead of calculating for each batch again and again. \n",
    "\n",
    "wav_dir = \"/data/lie_detection/clips_umich/audio\"\n",
    "save_dir = \"/data/lie_detection/clips_umich/wavlm_cnn\"\n",
    "mkdir(save_dir)\n",
    "for file in tqdm(get_files(wav_dir)):\n",
    "    fid = get_fid(file)\n",
    "    data, sr = sf.read(file)\n",
    "    data = torch.from_numpy(data).to(device).reshape(1, -1).float()\n",
    "    with torch.no_grad():\n",
    "        output = model(data, output_hidden_states=True)\n",
    "    \n",
    "    # TODO: Change the index here for features from different layers.\n",
    "    output = output.hidden_states[0].cpu().numpy()\n",
    "    np.save(os.path.join(save_dir, f\"{fid}.npy\"), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's process the video frames.\n",
    "# We want to extract the frames from the videos.\n",
    "video_dir = \"/data/lie_detection/clips_umich/video\"\n",
    "\n",
    "\n",
    "# Let's first check fps of each video. \n",
    "all_fps = []\n",
    "all_frames = []\n",
    "for file in get_files(video_dir):\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # if fps < 15:\n",
    "    #     print(file)\n",
    "    all_fps.append(fps)\n",
    "    all_frames.append(num_frames)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(all_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you could see, some videos have fps less than 15.\n",
    "# Micro-facial expression lasts less than 0.5 sec. \n",
    "# fps of 10 means the mfe will only have 3~5 frames, where 30fps will have 15 frames.\n",
    "\n",
    "# Also the audio feature is 50hz. \n",
    "# There're multiple ways to handle this.\n",
    "# I will do linear interpolation on the vision features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "device = torch.device('cuda:7')\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/120 [00:00<01:43,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_001.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 3/120 [00:02<01:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_002.wav: (1, 6373)\n",
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_003.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/120 [00:03<01:12,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_004.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/120 [00:04<01:40,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_005.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 6/120 [00:04<01:25,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_006.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7/120 [00:06<01:36,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_007.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/120 [00:06<01:12,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_008.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 9/120 [00:06<01:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_009.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10/120 [00:07<01:05,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_010.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/120 [00:08<01:11,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_011.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 12/120 [00:08<00:57,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_012.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 13/120 [00:08<00:53,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_013.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 14/120 [00:09<00:47,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_014.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 15/120 [00:09<00:56,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_015.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 16/120 [00:10<01:08,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_016.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 17/120 [00:11<01:18,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_017.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 18/120 [00:12<01:17,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_018.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 19/120 [00:13<01:16,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_019.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20/120 [00:13<01:02,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_020.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 21/120 [00:13<00:54,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_021.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 22/120 [00:14<01:05,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_022.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 23/120 [00:15<01:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_023.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 24/120 [00:16<01:05,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_024.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 25/120 [00:17<01:07,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_025.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 26/120 [00:17<01:06,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_026.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 27/120 [00:18<01:03,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_027.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 28/120 [00:19<00:59,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_028.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 29/120 [00:19<00:53,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_029.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 30/120 [00:20<00:57,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_030.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 31/120 [00:20<00:54,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_031.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 32/120 [00:21<00:54,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_032.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 33/120 [00:22<01:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_033.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 34/120 [00:23<00:59,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_034.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 35/120 [00:23<01:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_035.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 36/120 [00:24<01:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_036.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 37/120 [00:25<00:58,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_037.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 38/120 [00:26<01:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_038.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 39/120 [00:27<01:13,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_039.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 40/120 [00:28<01:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_040.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 41/120 [00:28<01:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_041.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 42/120 [00:29<01:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_042.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 43/120 [00:30<00:52,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_043.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 44/120 [00:30<00:43,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_044.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 45/120 [00:30<00:40,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_045.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 46/120 [00:31<00:42,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_046.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 47/120 [00:31<00:38,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_047.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 48/120 [00:33<00:51,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_048.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 49/120 [00:33<00:47,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_049.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 50/120 [00:34<00:45,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_050.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 51/120 [00:34<00:36,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_051.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 52/120 [00:35<00:45,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_052.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 53/120 [00:36<00:45,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_053.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 54/120 [00:36<00:39,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_054.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 55/120 [00:37<00:42,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_055.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 56/120 [00:37<00:41,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_056.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 57/120 [00:38<00:37,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_057.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 58/120 [00:39<00:36,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_058.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 59/120 [00:40<00:44,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_059.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 60/120 [00:40<00:41,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_lie_060.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 61/120 [00:41<00:35,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_001.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 62/120 [00:41<00:32,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_002.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 63/120 [00:41<00:27,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_003.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 64/120 [00:43<00:44,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_004.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 65/120 [00:44<00:41,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_005.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 66/120 [00:44<00:38,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_006.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 67/120 [00:46<00:48,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_007.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 68/120 [00:47<00:50,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_008.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 69/120 [00:47<00:43,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_009.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 70/120 [00:49<00:52,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_010.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 71/120 [00:50<00:48,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_011.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 72/120 [00:50<00:42,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_012.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 73/120 [00:51<00:38,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_013.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 74/120 [00:51<00:30,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_014.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 75/120 [00:53<00:47,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_015.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 76/120 [00:54<00:39,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_016.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 77/120 [00:54<00:32,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_017.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 78/120 [00:55<00:28,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_018.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 80/120 [00:55<00:18,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_019.wav: (1, 6373)\n",
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_020.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 81/120 [00:55<00:15,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_021.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 82/120 [00:56<00:18,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_022.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 83/120 [00:57<00:18,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_023.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 84/120 [00:57<00:18,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_024.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 85/120 [00:58<00:23,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_025.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 86/120 [01:00<00:30,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_026.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 87/120 [01:01<00:38,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_027.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 88/120 [01:03<00:39,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_028.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 89/120 [01:04<00:34,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_029.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 90/120 [01:05<00:33,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_030.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 91/120 [01:05<00:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_031.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 92/120 [01:06<00:25,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_032.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 93/120 [01:07<00:21,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_033.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 94/120 [01:07<00:20,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_034.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 95/120 [01:08<00:19,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_035.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 96/120 [01:09<00:20,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_036.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 97/120 [01:10<00:17,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_037.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 98/120 [01:10<00:15,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_038.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 99/120 [01:11<00:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_039.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 100/120 [01:12<00:14,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_040.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 101/120 [01:13<00:14,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_041.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 102/120 [01:13<00:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_042.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 103/120 [01:14<00:11,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_043.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 104/120 [01:14<00:10,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_044.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 105/120 [01:15<00:10,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_045.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 106/120 [01:16<00:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_046.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 107/120 [01:17<00:09,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_047.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 108/120 [01:17<00:08,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_048.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 109/120 [01:18<00:08,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_049.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 110/120 [01:19<00:07,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_050.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 111/120 [01:20<00:06,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_051.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 112/120 [01:21<00:06,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_052.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 113/120 [01:22<00:05,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_053.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 114/120 [01:22<00:04,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_054.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 115/120 [01:23<00:03,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_055.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 116/120 [01:23<00:02,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_056.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 117/120 [01:24<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_057.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 118/120 [01:25<00:01,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_058.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 119/120 [01:26<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_059.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [01:26<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y for file /data/lie_detection/clips_umich/audio/trial_truth_060.wav: (1, 6373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opensmile\n",
    "save_dir = \"/data/lie_detection/clips_umich/opensmile_extraction\"\n",
    "mkdir(save_dir)\n",
    "for file in tqdm(get_files(\"/data/lie_detection/clips_umich/audio\")):\n",
    "    data, sr = sf.read(file)\n",
    "    smile = opensmile.Smile(\n",
    "        feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "        feature_level=opensmile.FeatureLevel.Functionals,\n",
    "    )\n",
    "    fid = get_fid(file)\n",
    "    y = smile.process_file(file)\n",
    "    \n",
    "    # Print the shape of y\n",
    "    print(f\"Shape of y for file {file}: {y.shape}\")\n",
    "    \n",
    "    #y_flattened = y.values.flatten()\n",
    "    \n",
    "    # Print the shape of y_flattened\n",
    "    #print(f\"Shape of y_flattened for file {file}: {y_flattened.shape}\")\n",
    "    \n",
    "    np.save(os.path.join(save_dir, f\"{fid}.npy\"), y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ViT.py. \n",
    "save_dir = \"/data/lie_detection/clips_umich/viT\"\n",
    "mkdir(save_dir)\n",
    "\n",
    "# Save images.\n",
    "temp_dir = \"/data/lie_detection/clips_umich/temp\"\n",
    "mkdir(temp_dir)\n",
    "\n",
    "for file in get_files(\"/data/lie_detection/clips_umich/video\"):\n",
    "    save_path = os.path.join(save_dir, f\"{fid}.npy\")\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "    \n",
    "    fid = get_fid(file)\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    progress_bar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(temp_dir, f\"{count}.png\"), frame)     # save frame as PnG file\n",
    "        count += 1\n",
    "        progress_bar.update(1)\n",
    "    cap.release()\n",
    "    \n",
    "    images = [Image.open(i).convert('RGB') for i in get_files(temp_dir)]\n",
    "    frames = torch.stack([preprocess(i).to(device) for i in images])\n",
    "    with torch.no_grad():\n",
    "        output = model.encode_image(frames)\n",
    "    \n",
    "    np.save(save_path, output.cpu().numpy())\n",
    "    \n",
    "    # Clean up\n",
    "    for i in get_files(temp_dir):\n",
    "        os.remove(i)\n",
    "    assert len(get_files(temp_dir)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Please check the following pipeline for creating the dataset. \n",
    "# To properly create the split, we first check the length of each data. \n",
    "duration = []\n",
    "for file in get_files(\"/data/lie_detection/clips_umich/video\"):\n",
    "    # Open a file reader\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    \n",
    "    # Get fps.\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # Get total number of frames in the video.\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Calculate duration. \n",
    "    duration.append(num_frames/fps)\n",
    "    # Close the file reader. \n",
    "    cap.release()\n",
    "plt.hist(duration)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although the duration distribution is a little bit skewed, (not toooo far away from normal distribution)\n",
    "# we can still use random sampling (i.e. uniform distribution) to split the data.\n",
    "# This way we can ensure that the distribution of duration is similar in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 84\n",
      "Number of validation samples: 12\n",
      "Number of test samples: 24\n"
     ]
    }
   ],
   "source": [
    "# Let's follow a 80, 10, 10 split for now. \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fids = [get_fid(i) for i in get_files(\"/data/lie_detection/clips_umich/video\")]\n",
    "\n",
    "# Some lucky number. \n",
    "np.random.seed(888)\n",
    "\n",
    "# Shuffle the fids.\n",
    "random.shuffle(fids)\n",
    "\n",
    "# Split the fids.\n",
    "#train_fids = fids[:int(len(fids)*0.8)]\n",
    "#val_fids = fids[int(len(fids)*0.8):int(len(fids)*0.9)]\n",
    "#test_fids = fids[int(len(fids)*0.9):]\n",
    "n_total = len(fids)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "\n",
    "train_fids = fids[:n_train]\n",
    "val_fids = fids[n_train:n_train + n_val]\n",
    "test_fids = fids[n_train + n_val:]\n",
    "\n",
    "\n",
    "# Save the splits.\n",
    "write(\"/data/lie_detection/clips_umich/train.txt\", \"\\n\".join(train_fids))\n",
    "write(\"/data/lie_detection/clips_umich/val.txt\", \"\\n\".join(val_fids))\n",
    "write(\"/data/lie_detection/clips_umich/test.txt\", \"\\n\".join(test_fids))    \n",
    "\n",
    "print(f\"Number of training samples: {len(train_fids)}\")\n",
    "print(f\"Number of validation samples: {len(val_fids)}\")\n",
    "print(f\"Number of test samples: {len(test_fids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sample shape: torch.Size([1, 6885]), label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 6885]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, audio_dir, visual_dir, split_file):\n",
    "        # Split file only gives us file IDs. We still need data directories.\n",
    "        self.audio_dir = audio_dir\n",
    "        self.visual_dir = visual_dir\n",
    "        with open(split_file, 'r') as f:\n",
    "            fids = f.read().splitlines()\n",
    "        \n",
    "        # Get all files. This also ensures that \n",
    "        # getting index i from audio_files and visual_files will give us the file pointing to the same sample.\n",
    "        self.audio_files = [os.path.join(audio_dir, f\"{i}.npy\") for i in fids]\n",
    "        self.visual_files = [os.path.join(visual_dir, f\"{i}.npy\") for i in fids]\n",
    "\n",
    "        # Extract labels from file IDs\n",
    "        self.labels = [1 if 'lie' in fid.lower() else 0 for fid in fids]\n",
    "\n",
    "        # Ensure that these files exist. \n",
    "        for i in self.audio_files:\n",
    "            assert os.path.exists(i), f\"File not found: {i}\"\n",
    "        for i in self.visual_files:\n",
    "            assert os.path.exists(i), f\"File not found: {i}\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # We first read in the audio and visual features.\n",
    "        # (1, T, C). We squeeze the first dimension. => (T, C)\n",
    "        audio_features = np.load(self.audio_files[idx])\n",
    "        #audio_features = audio_features[0]\n",
    "        \n",
    "        # (T, C)\n",
    "        visual_features = np.load(self.visual_files[idx])\n",
    "        \n",
    "\n",
    "        \n",
    "        # Sometimes the features may be a few frames off, but no more than 5 frames.\n",
    "        # Take the minimum of the two lengths. (i.e., chunk the longer one to the length of the shorter one)\n",
    "        #assert abs(audio_features.shape[0] - visual_features.shape[0]) < 5, \"Audio and visual features should have frame difference less than 5.\"\n",
    "        \n",
    "        min_length = min(audio_features.shape[0], visual_features.shape[0])\n",
    "        audio_features = audio_features[:min_length]\n",
    "        visual_features = visual_features[:min_length]\n",
    "        \n",
    "        # Concatenate them on the last channel dimension. \n",
    "        concat_data = np.concatenate((audio_features, visual_features), axis=-1)\n",
    "        \n",
    "        # Convert the numpy array to a PyTorch tensor.\n",
    "        concat_data = torch.from_numpy(concat_data).float()\n",
    "        \n",
    "        # Get the label for this sample\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return concat_data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the data and labels\n",
    "    data, labels = zip(*batch)\n",
    "    \n",
    "    # We will pad the sequence with 0s to the maximum length in the batch.\n",
    "    max_len = max([i.shape[0] for i in data])\n",
    "    padded_batch = []\n",
    "    for i in data:\n",
    "        pad_len = max_len - i.shape[0]\n",
    "        padded_batch.append(torch.nn.functional.pad(i, (0, 0, 0, pad_len)))\n",
    "    \n",
    "    # Now all samples have the same length. \n",
    "    # We can stack them into a tensor.\n",
    "    # Shape: (batch_size, T, C)\n",
    "    data = torch.stack(padded_batch)\n",
    "    \n",
    "    # Stack labels into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Now let's test the dataset.\n",
    "audio_dir = \"/data/lie_detection/clips_umich/opensmile_extraction\"\n",
    "visual_dir = \"/data/lie_detection/clips_umich/viT_50hz\"\n",
    "split_file = \"/data/lie_detection/clips_umich/train.txt\"\n",
    "\n",
    "dataset = MultiModalDataset(audio_dir, visual_dir, split_file)\n",
    "print(f\"dataset sample shape: {dataset[0][0].shape}, label: {dataset[0][1]}\")\n",
    "\n",
    "# We don't need further processing. Can just use the default dataloader.\n",
    "# Always set shuffle to True for training. False for validation and testing.\n",
    "# Adjust batch_size and num_workers based on your GPU memory and CPU count. \n",
    "\n",
    "######### IMPORTANT #########\n",
    "# Don't set num_workers above 8. There're only 64 threads in total on the server.\n",
    "# num_workers of 2 or 4 should be good enough for most cases.\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# Now let's test the dataloader.\n",
    "# iter function will return an iterator.\n",
    "# next function will return the next batch.\n",
    "\n",
    "# Batch will have a shape of (batch_size, T, C). \n",
    "# C = 1024 + 512 = 1536\n",
    "# T is the maximum length in the batch.\n",
    "batch = next(iter(dataloader))\n",
    "print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only audio loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VisualDataset(Dataset):\n",
    "    def __init__(self, visual_dir, split_file):\n",
    "        # Split file only gives us file IDs. We still need data directories.\n",
    "        self.visual = visual_dir\n",
    "        with open(split_file, 'r') as f:\n",
    "            fids = f.read().splitlines()\n",
    "        self.visual_files = [os.path.join(visual_dir, f\"{i}.npy\") for i in fids]\n",
    "        self.labels = [1 if 'lie' in fid.lower() else 0 for fid in fids]\n",
    "\n",
    "        for i in self.visual_files:\n",
    "            assert os.path.exists(i), f\"File not found: {i}\"\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # We first read in the audio and visual features.\n",
    "        # (1, T, C). We squeeze the first dimension. => (T, C)\n",
    "        visual_features = np.load(self.visual_files[idx])\n",
    "        #audio_features = audio_features[0]\n",
    "        \n",
    "        # Sometimes the features may be a few frames off, but no more than 5 frames.\n",
    "        # Take the minimum of the two lengths. (i.e., chunk the longer one to the length of the shorter one)\n",
    "        #assert abs(audio_features.shape[0] - visual_features.shape[0]) < 5, \"Audio and visual features should have frame difference less than 5.\"\n",
    "        # Concatenate them on the last channel dimension. \n",
    "        #concat_data = np.concatenate((audio_features, visual_features), axis=-1)\n",
    "        dataset = visual_features\n",
    "        # Convert the numpy array to a PyTorch tensor.\n",
    "        dataset = torch.from_numpy(dataset).float()\n",
    "        # Get the label for this sample\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return dataset, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.visual_files)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the data and labels\n",
    "    data, labels = zip(*batch)\n",
    "    # We will pad the sequence with 0s to the maximum length in the batch.\n",
    "    max_len = max([i.shape[0] for i in data])\n",
    "    padded_batch = []\n",
    "    for i in data:\n",
    "        pad_len = max_len - i.shape[0]\n",
    "        padded_batch.append(torch.nn.functional.pad(i, (0, 0, 0, pad_len)))\n",
    "    \n",
    "    # Now all samples have the same length. \n",
    "    # We can stack them into a tensor.\n",
    "    # Shape: (batch_size, T, C)\n",
    "    data = torch.stack(padded_batch)\n",
    "    \n",
    "    # Stack labels into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Now let's test the dataset.\n",
    "visual_dir = \"/data/lie_detection/clips_umich/viT_50hz\"\n",
    "split_file = \"/data/lie_detection/clips_umich/train.txt\"\n",
    "\n",
    "dataset = VisualDataset(audio_dir, split_file)\n",
    "print(f\"dataset sample shape: {dataset[0][0].shape}, label: {dataset[0][1]}\")\n",
    "\n",
    "# We don't need further processing. Can just use the default dataloader.\n",
    "# Always set shuffle to True for training. False for validation and testing.\n",
    "# Adjust batch_size and num_workers based on your GPU memory and CPU count. \n",
    "\n",
    "######### IMPORTANT #########\n",
    "# Don't set num_workers above 8. There're only 64 threads in total on the server.\n",
    "# num_workers of 2 or 4 should be good enough for most cases.\n",
    "visual_dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# Now let's test the dataloader.\n",
    "# iter function will return an iterator.\n",
    "# next function will return the next batch.\n",
    "\n",
    "# Batch will have a shape of (batch_size, T, C). \n",
    "# C = 1024 + 512 = 1536\n",
    "# T is the maximum length in the batch.\n",
    "batch = next(iter(visual_dataloader))\n",
    "print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1349, 0.7725, 0.3627, 0.4999, 0.9895, 0.0144, 0.8421, 0.8704],\n",
       "         [0.4715, 0.5765, 0.5355, 0.4579, 0.1864, 0.7845, 0.2348, 0.8641],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       " torch.Size([5, 8]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here's quick example of how padding works.\n",
    "# Let's say the current data has shape (T, C)\n",
    "# We want to pad the T to 5. \n",
    "data = torch.rand((2, 8))\n",
    "result = torch.nn.functional.pad(data, (0, 0, 0, 3))\n",
    "\n",
    "result, result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "    #save as truth  \n",
    "\n",
    "# Be sure to convert the model & data to the correct device.\n",
    "# Evaluate the model on the validation set.\n",
    "# Save the model with the best validation performance.\n",
    "# Test the model with the best validation performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual feature interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "############### 1. Load the video fps ###############\n",
    "video_dir = \"/data/lie_detection/clips_umich/video\"\n",
    "\n",
    "\n",
    "# Let's first check fps of each video. \n",
    "all_fps = {}\n",
    "all_frames = []\n",
    "for file in get_files(video_dir):\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # if fps < 15:\n",
    "    #     print(file)\n",
    "    all_fps[get_fid(file)] = fps\n",
    "    all_frames.append(num_frames)\n",
    "    cap.release()\n",
    "    \n",
    "############### 2. Interpolate visual features ###############\n",
    "device = torch.device('cuda:7')\n",
    "\n",
    "# We will save the interpolated features in this folder.\n",
    "save_dir = \"/data/lie_detection/clips_umich/viT_50hz\"\n",
    "mkdir(save_dir)\n",
    "\n",
    "# To get all files in a folder in a sorted order, just use 'get_files(folder)'\n",
    "# tqdm simply adds a progress bar. \n",
    "for file in tqdm(get_files('/data/lie_detection/clips_umich/viT')):\n",
    "    # Get the file id. We want to save the file with the same id.\n",
    "    fid = get_fid(file)\n",
    "    fps = all_fps[fid]\n",
    "    \n",
    "    # Interpolate the frames to 50hz. \n",
    "    # (T, C)\n",
    "    # Want (B, C, T) as input shape. to the interpolate. \n",
    "    # Use GPU so that it's faster.\n",
    "    data = np.load(file)\n",
    "    data = torch.from_numpy(data).to(device).float()\n",
    "    \n",
    "    # Convert input shape from (T, C) => (1, T, C) => (1, C, T)\n",
    "    data = data.unsqueeze(0).transpose(1, 2)\n",
    "    target_len = int(data.shape[2] * 50 / fps)\n",
    "    data = torch.nn.functional.interpolate(data, size=target_len, mode='linear', align_corners=False)\n",
    "    data = data.squeeze(0).transpose(0, 1)\n",
    "    \n",
    "    # Save data.\n",
    "    # Before save, move data back to cpu else it will error.\n",
    "    np.save(os.path.join(save_dir, f\"{fid}.npy\"), data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check. \n",
    "#data = np.load(\"/data/lie_detection/clips_umich/viT_50hz/trial_lie_001.npy\")\n",
    "#previous_data = np.load(\"/data/lie_detection/clips_umich/viT/trial_lie_001.npy\")\n",
    "\n",
    "print(f\"After Interpolation data shape: {data.shape}\")\n",
    "print(f\"Before interpolation data shape: {previous_data.shape}\")\n",
    "\n",
    "print(f\"Previous video fps: {all_fps['trial_lie_001']}\")\n",
    "\n",
    "# You can see that the length indeed match 50hz. \n",
    "510 / 29.97 * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiLM Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(ic, oc, k, s, p):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv1d(ic, oc, k, s, p),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(oc),\n",
    "    )\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            conv1d(in_channels, 128, 5, 2, 2),\n",
    "            conv1d(128, 128, 3, 2, 1),\n",
    "            conv1d(128, 128, 3, 2, 1),\n",
    "            conv1d(128, 128, 3, 1, 1),\n",
    "            conv1d(128, 128, 3, 1, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class FiLMBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FiLMBlock, self).__init__()\n",
    "        \n",
    "    def forward(self, x, gamma, beta):\n",
    "        beta = beta.view(x.size(0), x.size(1), 1)\n",
    "        gamma = gamma.view(x.size(0), x.size(1), 1)\n",
    "        \n",
    "        x = gamma * x + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_place, out_place):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_place, out_place, 1, 1, 0)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.norm1 = nn.BatchNorm1d(out_place)\n",
    "        self.conv2 = nn.Conv1d(out_place, out_place, 3, 1, 1)\n",
    "        self.norm2 = nn.BatchNorm1d(out_place)\n",
    "        self.film = FiLMBlock()\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, beta, gamma):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        identity = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.film(x, gamma, beta)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = x + identity\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, prev_channels, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(prev_channels, 512, 1, 1, 0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.model = nn.Sequential(nn.Linear(512, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, 1024),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Linear(1024, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.global_max_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, n_res_blocks, n_classes, n_channels, in_channels):\n",
    "        super(FiLM, self).__init__()\n",
    "        dim_question = 11\n",
    "        self.film_generator = nn.Linear(dim_question, 2 * n_res_blocks * n_channels)\n",
    "        self.feature_extractor = FeatureExtractor(in_channels)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(n_res_blocks):\n",
    "            self.res_blocks.append(ResBlock(n_channels + 2, n_channels))\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer\n",
    "        self.classifier = Classifier(n_channels, n_classes)\n",
    "    \n",
    "        self.n_res_blocks = n_res_blocks\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "    def forward(self, x, question):\n",
    "        batch_size = x.size(0)\n",
    "        if x.dim() == 5:\n",
    "            # Remove extra dimensions\n",
    "            x = x.squeeze(-1).squeeze(-1)\n",
    "        x = self.feature_extractor(x)\n",
    "        film_vector = self.film_generator(question).view(\n",
    "            batch_size, self.n_res_blocks, 2, self.n_channels)\n",
    "        \n",
    "        d = x.size(2)\n",
    "        coordinate = torch.linspace(-1, 1, d).to(x.device)\n",
    "        coordinate_x = coordinate.view(1, 1, d).expand(batch_size, 1, d)\n",
    "        coordinate_y = coordinate.view(1, 1, d).expand(batch_size, 1, d)\n",
    "        \n",
    "        for i, res_block in enumerate(self.res_blocks):\n",
    "            beta = film_vector[:, i, 0, :].unsqueeze(2)\n",
    "            gamma = film_vector[:, i, 1, :].unsqueeze(2)\n",
    "            \n",
    "            x = torch.cat([x, coordinate_x, coordinate_y], 1)\n",
    "            x = res_block(x, beta, gamma)\n",
    "        \n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "def make_model(model_params):\n",
    "    return FiLM(**model_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
